{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "#Imports from this projects\n",
    "import auxiliary.util as util\n",
    "util.set_wd_to_package_root()\n",
    "import auxiliary.config as config\n",
    "import auxiliary.grid2op_util as g2o_util\n",
    "from auxiliary.generate_action_space import action_identificator\n",
    "import data_preprocessing_analysis.imitation_data_preprocessing as idp\n",
    "\n",
    "#Mathematics\n",
    "import math\n",
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "from scipy.spatial import distance\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#Collections\n",
    "import collections\n",
    "from collections import Counter\n",
    "\n",
    "#File manipulation\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd #Data manipulation & analsysis\n",
    "import grid2op #Grid simulation\n",
    "import matplotlib.pyplot as plt #Plotting\n",
    "import ipdb #Debugger\n",
    "import re #Regular expressions\n",
    "import functools #Higher-order functions\n",
    "from tqdm import tqdm #Progress bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = config.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data_path = config['paths']['processed_tutor_imitation']\n",
    "con_matrix_path = config['paths']['con_matrix_cache']\n",
    "fstats_path = config['paths']['feature_statistics']\n",
    "\n",
    "line_disabled_to_consider = [-1,0,1,2,3,4,5,6,10,12,13,15,16,19]\n",
    "line_group1 = [-1,0,1,2,3,4,5,6,12]\n",
    "line_group2 = [13,15,16,19]\n",
    "chronics_excluded = [310, 446, 777]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/processed_tutor_data/'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config['paths']['processed_tutor_imitation']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processed Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next block defines several data aggregates, such as counters. The processed data is loaded a file at a time, gradually filling the data aggregates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 997/997 [00:12<00:00, 81.22it/s] \n"
     ]
    }
   ],
   "source": [
    "n_sub = 14\n",
    "\n",
    "#Inbstantiate the counter objects\n",
    "counters = {}\n",
    "for i in np.arange(-1,20):\n",
    "    counters[i] = {\n",
    "        'n_datapoints':0,\n",
    "        'n_days_completed':0,\n",
    "        'n_chronics':0,\n",
    "        'set_hash': collections.Counter(),\n",
    "        'res_hash': collections.Counter(),\n",
    "        'tv_hash': collections.Counter(),\n",
    "        'sub_changed': (n_sub+1) * [0],\n",
    "        'changed_subs_n': n_sub * [0],\n",
    "        'sub_info': []\n",
    "    }\n",
    "        \n",
    "# To count numpy arrays, we index their counters with hashes, stored in hash dictionaries:\n",
    "hash_to_act = {} #'Set'-action hashes\n",
    "hash_to_tv = {} #Topology vector hashes\n",
    "hash_to_res = {} #Resulting (i.e. post-action) topology vector hashes\n",
    "\n",
    "\n",
    "for f in tqdm(list(Path(processed_data_path).rglob('*.json'))):\n",
    "    with open(f, 'r') as file:\n",
    "            dps = json.loads(file.read())\n",
    "            \n",
    "    line_disabled = dps[0]['line_disabled']\n",
    "    \n",
    "    counters[line_disabled]['n_chronics']+=1\n",
    "    counters[line_disabled]['n_days_completed']+=dps[0]['dayscomp']\n",
    "    for dp in dps:\n",
    "        #Increase n. datapoints\n",
    "        counters[line_disabled]['n_datapoints']+=1\n",
    "        \n",
    "        #Count set_topo_vect\n",
    "        hsh_set = util.hash_nparray(np.array(dp['set_topo_vect']))\n",
    "        if hsh_set not in hash_to_act:\n",
    "            hash_to_act[hsh_set] = dp['set_topo_vect']\n",
    "        counters[line_disabled]['set_hash'][hsh_set]+=1\n",
    "        \n",
    "        #Count res_topo_vect\n",
    "        hsh_res = util.hash_nparray(np.array(dp['res_topo_vect']))\n",
    "        if hsh_res not in hash_to_res:\n",
    "            hash_to_res[hsh_res] = dp['res_topo_vect']\n",
    "        counters[line_disabled]['res_hash'][hsh_res]+=1\n",
    "        \n",
    "        #Count topo_vect\n",
    "        hsh_tv = util.hash_nparray(np.array(dp['topo_vect']))\n",
    "        if hsh_tv not in hash_to_tv:\n",
    "            hash_to_tv[hsh_tv] = dp['topo_vect']\n",
    "        counters[line_disabled]['tv_hash'][hsh_tv]+=1\n",
    "        \n",
    "        #Count substations affected\n",
    "        action_per_sub = g2o_util.tv_groupby_subst(dp['set_topo_vect'],dp['sub_info'])\n",
    "        try:\n",
    "            changed_subs_id = [np.any(a) for i,a in enumerate(action_per_sub)].index(True)\n",
    "            counters[line_disabled]['sub_changed'][changed_subs_id] += 1\n",
    "        except:\n",
    "            counters[line_disabled]['sub_changed'][-1] += 1\n",
    "\n",
    "        #Count topological depth of resulting topologies\n",
    "        #ASSUMPTION: reference topology is the topology where all objects are connected to bus 1\n",
    "        res_per_sub = g2o_util.tv_groupby_subst(dp['res_topo_vect'],dp['sub_info'])\n",
    "        changed_subs_n = sum([2 in res for i,res in enumerate(res_per_sub)])\n",
    "        counters[line_disabled]['changed_subs_n'][changed_subs_n] += 1\n",
    "        \n",
    "        #Set sub info\n",
    "        counters[line_disabled]['sub_info'] = dp['sub_info']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of chronics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "997"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([v['n_chronics']for k,v in counters.items() if k in line_disabled_to_consider])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Percentage days completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(-1, 1.0), None, None, None, None, None, None, None, None, None, None, None, None, None]\n"
     ]
    }
   ],
   "source": [
    "print([(k,v['n_days_completed']/(28*v['n_chronics'])) if v['n_chronics']!=0 else None\n",
    "       for k,v in counters.items() \n",
    "       if k in line_disabled_to_consider])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 1.0\n",
      "Group 1: 1.0\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-0f3c4270fe6d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     sum([28*v['n_chronics']for k,v in counters.items() if k in line_group1]))\n\u001b[1;32m      7\u001b[0m print('Group 2:',\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'n_days_completed'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcounters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mline_group2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     sum([28*v['n_chronics']for k,v in counters.items() if k in line_group2]))\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "print('Total:',\n",
    "    sum([v['n_days_completed']for k,v in counters.items() if k in line_disabled_to_consider])/ \\\n",
    "    sum([28*v['n_chronics']for k,v in counters.items() if k in line_disabled_to_consider]))\n",
    "print('Group 1:',\n",
    "    sum([v['n_days_completed']for k,v in counters.items() if k in line_group1])/ \\\n",
    "    sum([28*v['n_chronics']for k,v in counters.items() if k in line_group1]))\n",
    "print('Group 2:',\n",
    "    sum([v['n_days_completed']for k,v in counters.items() if k in line_group2])/ \\\n",
    "    sum([28*v['n_chronics']for k,v in counters.items() if k in line_group2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(k,v['n_datapoints'])for k,v in counters.items() if k in line_disabled_to_consider]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_total = sum([v['n_datapoints']for k,v in counters.items() if k in line_disabled_to_consider])\n",
    "N_group1 = sum([v['n_datapoints']for k,v in counters.items() if k in line_group1])\n",
    "N_group2 = sum([v['n_datapoints']for k,v in counters.items() if k in line_group2])\n",
    "\n",
    "print('Total:', N_total)\n",
    "print('Group 1:', N_group1)\n",
    "print('Group 2:', N_group2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(line_disabled_to_consider,\n",
    "        [counters[l]['n_datapoints'] for l in np.arange(-1,20) if l in line_disabled_to_consider])\n",
    "ax = plt.gca()\n",
    "_ = ax.set_xticks(np.arange(-1,20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Action statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(k,len(v['set_hash']))\n",
    " for k,v in counters.items() \n",
    " if k in line_disabled_to_consider]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Percentage of do-nothing actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the hashes that are do-nothing actions. Should not be higher than two. \n",
    "do_nothing_action_hashes = [h for h,t \n",
    "                            in hash_to_act.items() \n",
    "                            if sum(t)==0]\n",
    "assert len(do_nothing_action_hashes) < 3\n",
    "\n",
    "[(k,sum([v['set_hash'][h] for h in do_nothing_action_hashes])/v['n_datapoints'])\n",
    " for k,v in counters.items() \n",
    " if k in line_disabled_to_consider and v['n_datapoints'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([sum([v['set_hash'][h] for h in do_nothing_action_hashes])\n",
    "          /v['n_datapoints']\n",
    "         for k,v in counters.items() \n",
    "         if k in line_disabled_to_consider and v['n_datapoints'] != 0])/N_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total:',\n",
    "     sum([sum([v['set_hash'][h] for h in do_nothing_action_hashes])\n",
    "         for k,v in counters.items() \n",
    "         if k in line_disabled_to_consider and v['n_datapoints'] != 0])/N_total)\n",
    "print('Group 1:',\n",
    "     sum([sum([v['set_hash'][h] for h in do_nothing_action_hashes])\n",
    "         for k,v in counters.items() \n",
    "         if k in line_group1 and v['n_datapoints'] != 0])/N_group1)\n",
    "print('Group 2:',\n",
    "     sum([sum([v['set_hash'][h] for h in do_nothing_action_hashes])\n",
    "         for k,v in counters.items() \n",
    "         if k in line_group2 and v['n_datapoints'] != 0])/N_group2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entropy of the action distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(k,entropy(list(v['set_hash'].values()))) \n",
    " for k,v in counters.items() \n",
    " if k in line_disabled_to_consider]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting actions into format so that actions at substations\n",
    "standard_sub_info = [3, 6, 4, 6, 5, 6, 3, 2, 5, 3, 3, 3, 4, 3]\n",
    "    \n",
    "for i in np.arange(-1,20):\n",
    "    act_counter = counters[i]['set_hash']\n",
    "    unique_act_counter = collections.Counter()\n",
    "    for h,c in act_counter.items():\n",
    "        a = hash_to_act[h]\n",
    "        a_per_substation = g2o_util.tv_groupby_subst(a,counters[i]['sub_info'])\n",
    "        try:\n",
    "            changed_subs_id = [np.any(a) for a in a_per_substation].index(True)\n",
    "            action = (changed_subs_id,tuple(a_per_substation[changed_subs_id]),\n",
    "                      None if standard_sub_info[changed_subs_id]==counters[i]['sub_info'][changed_subs_id] else i)\n",
    "            unique_act_counter[action] += c\n",
    "        except ValueError:\n",
    "            unique_act_counter[-1] += c\n",
    "    counters[i]['unique_set_act'] = unique_act_counter\n",
    "    \n",
    "combined_act_counter = collections.Counter()\n",
    "act_counter_group1 = collections.Counter()\n",
    "act_counter_group2 = collections.Counter()\n",
    "for i in np.arange(-1,20):\n",
    "    combined_act_counter = combined_act_counter + counters[i]['unique_set_act']\n",
    "    if i in line_group1:\n",
    "        act_counter_group1 += act_counter_group1 + counters[i]['unique_set_act']\n",
    "    elif  i in line_group2:\n",
    "        act_counter_group2 += act_counter_group2 + counters[i]['unique_set_act']\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tups = list(combined_act_counter.keys())\n",
    "tups = sorted([t for t in tups if type(t)==tuple])\n",
    "tups = [-1] + tups\n",
    "tups\n",
    "\n",
    "colormap = {\n",
    "    -1:'k',\n",
    "    1:'b',\n",
    "    2:'g',\n",
    "    3:'r',\n",
    "    4:'c',\n",
    "    5:'m',\n",
    "    8:'y',\n",
    "    12:'k',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(4,4,figsize=[2*6.4, 2*4.8])#, sharex=True, sharey=True)\n",
    "axs = axs.reshape(-1)\n",
    "fig.tight_layout()\n",
    "\n",
    "for plt_i, c_i in enumerate(line_disabled_to_consider):\n",
    "    act_counter = counters[c_i]['unique_set_act']\n",
    "    weight = [act_counter[i] for i in tups]\n",
    "\n",
    "    _, _, patches = axs[plt_i].hist(range(len(weight)), weights=weight,bins=range(len(weight)))\n",
    "    axs[plt_i].title.set_text(f'Line {c_i} disabled.' if c_i>-1 else 'No line disabled.')\n",
    "    \n",
    "    #Applying colors\n",
    "    for j,t in enumerate(tups[:-1]):\n",
    "        if type(t) == int:\n",
    "            continue\n",
    "        patches[j].set_facecolor(colormap[t[0]])\n",
    "    patches[0].set_facecolor(colormap[-1])\n",
    "    \n",
    "    axs[plt_i].ticklabel_format(axis=\"y\", style=\"sci\", scilimits=(0,0))\n",
    "\n",
    "for i in range(len(axs)-len(line_disabled_to_consider)):\n",
    "    axs[len(axs)-i-1].axis('off')\n",
    "    \n",
    "_ = fig.suptitle('Resulting \\'Set\\' Action Distribution per Topology', fontsize=16, y=1.05)\n",
    "fig.savefig('data_preprocessing_analysis/figures/action_distribution_per_topology.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counters[-1]['set_hash'].most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = [combined_act_counter[i] for i in tups]\n",
    "_, _, patches = plt.hist(range(len(weight)), weights=weight,bins=range(len(weight)))\n",
    "\n",
    "#Applying colors\n",
    "for j,t in enumerate(tups[:-1]):\n",
    "    if type(t) == int:\n",
    "        continue\n",
    "    patches[j].set_facecolor(colormap[t[0]])\n",
    "patches[0].set_facecolor(colormap[-1])\n",
    "plt.ticklabel_format(axis=\"y\", style=\"sci\", scilimits=(0,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = [c for v,c in counters[-1]['set_hash'].most_common()]\n",
    "_, _, patches = plt.hist(range(len(weight)), weights=weight,bins=range(len(weight)))\n",
    "\n",
    "#Applying colors\n",
    "#patches[0].set_facecolor(colormap[-1])\n",
    "#plt.ticklabel_format(axis=\"y\", style=\"sci\", scilimits=(0,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total:',\n",
    "    len(combined_act_counter.most_common()),\n",
    "      entropy(list(combined_act_counter.values())))\n",
    "print('Group 1:',\n",
    "      len(act_counter_group1.most_common()),\n",
    "      entropy(list(act_counter_group1.values())))\n",
    "print('Group 2:',\n",
    "      len(act_counter_group2.most_common()),\n",
    "      entropy(list(act_counter_group2.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[2*6.4, 2*4.8], dpi=80)\n",
    "\n",
    "data = np.random.rand(len(line_disabled_to_consider), len(line_disabled_to_consider))\n",
    "for iy,y in enumerate([counters[i]['unique_set_act'] for i in line_disabled_to_consider]):\n",
    "    for ix,x in enumerate([counters[i]['unique_set_act'] for i in line_disabled_to_consider]):\n",
    "        cosine = distance.cosine([y[a] for a in combined_act_counter.keys()],\n",
    "                                     [x[a] for a in combined_act_counter.keys()])\n",
    "        data[iy,ix] = cosine\n",
    "heatmap = plt.pcolor(data,cmap='viridis_r')\n",
    "\n",
    "for y in range(data.shape[0]):\n",
    "    for x in range(data.shape[1]):\n",
    "        plt.text(x + 0.5, y + 0.5, '%.4f' % data[y, x],\n",
    "                 horizontalalignment='center',\n",
    "                 verticalalignment='center',\n",
    "                 )\n",
    "\n",
    "plt.colorbar(heatmap)\n",
    "plt.xticks(np.arange(0.5,len(line_disabled_to_consider)+0.5),['None']+line_disabled_to_consider[1:])\n",
    "plt.yticks(np.arange(0.5,len(line_disabled_to_consider)+0.5),['None']+line_disabled_to_consider[1:])\n",
    "plt.ylabel('Line disabled')\n",
    "plt.xlabel('Line disabled')\n",
    "plt.title('Cosine distance between the action distributions of the different topologies.')\n",
    "plt.savefig('data_preprocessing_analysis/figures/cosine_distance_actions.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(len(combined_act_counter.most_common()),entropy(list(combined_act_counter.values())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topology Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(4,4,figsize=[2*6.4, 2*4.8])#, sharex=True, sharey=True)\n",
    "axs = axs.reshape(-1)\n",
    "fig.tight_layout()\n",
    "for plt_i, c_i in enumerate(line_disabled_to_consider):\n",
    "    res_counter = counters[c_i]['tv_hash']\n",
    "    if not res_counter:\n",
    "        continue\n",
    "    val, weight = zip(*[(i, v) for i,(k,v) in enumerate(res_counter.most_common())])\n",
    "    axs[plt_i].hist(val[0:100], weights=weight[0:100],bins=val[0:100])\n",
    "    axs[plt_i].title.set_text(f'Line {c_i} removed')\n",
    "\n",
    "for i in range(len(axs)-len(line_disabled_to_consider)):\n",
    "    axs[len(axs)-i-1].axis('off')\n",
    "    \n",
    "_ = fig.suptitle('Topology vector distribution per line outage', fontsize=16, y=1.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(4,4,figsize=[2*6.4, 2*4.8])#, sharex=True, sharey=True)\n",
    "axs = axs.reshape(-1)\n",
    "fig.tight_layout()\n",
    "for plt_i, c_i in enumerate(line_disabled_to_consider):\n",
    "    res_counter = counters[c_i]['res_hash']\n",
    "    if not res_counter:\n",
    "        continue\n",
    "    val, weight = zip(*[(i, v) for i,(k,v) in enumerate(res_counter.most_common())])\n",
    "    axs[plt_i].hist(val[0:100], weights=weight[0:100],bins=val[0:100])\n",
    "    axs[plt_i].title.set_text(f'Line {c_i} removed')\n",
    "\n",
    "for i in range(len(axs)-len(line_disabled_to_consider)):\n",
    "    axs[len(axs)-i-1].axis('off')\n",
    "    \n",
    "_ = fig.suptitle('Resulting topology distribution per line outage', fontsize=16, y=1.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counters[-1]['res_hash'].most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Substations acted on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "substations_with_actions = [1,2,3,4,5,8,12,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(counters[c_i]['sub_changed'])[substations_with_actions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(4,4,figsize=[2*6.4, 2*4.8])#, sharex=True, sharey=True)\n",
    "axs = axs.reshape(-1)\n",
    "fig.tight_layout()\n",
    "for plt_i, c_i in enumerate(line_disabled_to_consider):\n",
    "    axs[plt_i].bar([str(b) for b in substations_with_actions],\n",
    "                   np.array(counters[c_i]['sub_changed'])[substations_with_actions])\n",
    "    axs[plt_i].title.set_text(f'Line {c_i} removed')\n",
    "\n",
    "for i in range(len(axs)-len(line_disabled_to_consider)):\n",
    "    axs[len(axs)-i-1].axis('off')\n",
    "    \n",
    "_ = fig.suptitle('Substations acted on per line outage', fontsize=16, y=1.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,3,figsize=[3*6.4, 1*4.8])\n",
    "\n",
    "axs[0].bar([str(l) for l in substations_with_actions[:-1]] + ['No action'],\n",
    "        np.sum(np.array([counters[c_i]['sub_changed'] \\\n",
    "                                                   for c_i in line_group1]),axis=0)[substations_with_actions])\n",
    "axs[0].ticklabel_format(axis=\"y\", style=\"sci\", scilimits=(0,0))\n",
    "axs[0].set_title('Cluster 1')\n",
    "\n",
    "axs[1].bar([str(l) for l in substations_with_actions[:-1]] + ['No action'],\n",
    "        np.sum(np.array([counters[c_i]['sub_changed'] \\\n",
    "                                                   for c_i in line_group2]),axis=0)[substations_with_actions])\n",
    "axs[1].ticklabel_format(axis=\"y\", style=\"sci\", scilimits=(0,0))\n",
    "axs[1].set_title('Cluster 2')\n",
    "\n",
    "axs[2].bar([str(l) for l in substations_with_actions[:-1]] + ['No action'],\n",
    "        np.sum(np.array([counters[c_i]['sub_changed'] \\\n",
    "                                                   for c_i in line_disabled_to_consider]),axis=0)[substations_with_actions])\n",
    "axs[2].ticklabel_format(axis=\"y\", style=\"sci\", scilimits=(0,0))\n",
    "axs[2].set_title('Total')\n",
    "\n",
    "fig.suptitle('Distribution of Substations Acted On', fontsize=16, y=1)\n",
    "fig.savefig('data_preprocessing_analysis/figures/distribution_substations.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar([str(l) for l in substations_with_actions[:-1]] + ['No action'],\n",
    "        np.sum(np.array([counters[c_i]['sub_changed'] \\\n",
    "                                                   for c_i in line_disabled_to_consider]),axis=0)[substations_with_actions])\n",
    "#plt.title('Substations Acted On')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topological Depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_nothing_action_hashes = [h for h,t in hash_to_act.items() if sum(t)==0]\n",
    "def mean_index(lst):\n",
    "    return np.sum(np.array([i*v for i,v in enumerate(lst)]))/sum(lst)\n",
    "[(k,mean_index(v['changed_subs_n']))\n",
    " for k,v in counters.items() if k in line_disabled_to_consider]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mean topological depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total:',\n",
    "    mean_index(np.sum(np.array([v['changed_subs_n'] for k,v in counters.items() \n",
    "                            if k in line_disabled_to_consider ]),axis=0)))\n",
    "print('Group 1:',\n",
    "      mean_index(np.sum(np.array([v['changed_subs_n'] for k,v in counters.items() \n",
    "                            if k in line_group1 ]),axis=0)))\n",
    "print('Group 2:',\n",
    "    mean_index(np.sum(np.array([v['changed_subs_n'] for k,v in counters.items() \n",
    "                            if k in line_group2 ]),axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(4,4,figsize=[2*6.4, 2*4.8])#, sharex=True, sharey=True)\n",
    "axs = axs.reshape(-1)\n",
    "fig.tight_layout()\n",
    "for plt_i, c_i in enumerate(line_disabled_to_consider):\n",
    "    axs[plt_i].bar([str(n) for n in range(len(substations_with_actions))],\n",
    "                   counters[c_i]['changed_subs_n'][0:len(substations_with_actions)])\n",
    "    axs[plt_i].title.set_text(f'Line {c_i} removed')\n",
    "\n",
    "for i in range(len(axs)-len(line_disabled_to_consider)):\n",
    "    axs[len(axs)-i-1].axis('off')\n",
    "    \n",
    "_ = fig.suptitle('Topological depth of resulting topologies per line outage', fontsize=16, y=1.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar([str(n) for n in range(n_sub)],np.sum(np.array([counters[c_i]['changed_subs_n'] \\\n",
    "                                                        for c_i in np.arange(-1,20)]),axis=0))\n",
    "plt.title('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,3,figsize=[3*6.4, 1*4.8])\n",
    "\n",
    "axs[0].bar(range(len(substations_with_actions)),\n",
    "        np.sum(np.array([counters[c_i]['changed_subs_n'] \\\n",
    "                                                   for c_i in line_group1]),axis=0)[0:8])\n",
    "axs[0].ticklabel_format(axis=\"y\", style=\"sci\", scilimits=(0,0))\n",
    "axs[0].set_title('Cluster 1')\n",
    "\n",
    "axs[1].bar(range(len(substations_with_actions)),\n",
    "        np.sum(np.array([counters[c_i]['changed_subs_n'] \\\n",
    "                                                   for c_i in line_group2]),axis=0)[0:8])\n",
    "axs[1].ticklabel_format(axis=\"y\", style=\"sci\", scilimits=(0,0))\n",
    "axs[1].set_title('Cluster 2')\n",
    "\n",
    "axs[2].bar(range(len(substations_with_actions)),\n",
    "        np.sum(np.array([counters[c_i]['changed_subs_n'] \\\n",
    "                                                   for c_i in line_disabled_to_consider]),axis=0)[0:8])\n",
    "axs[2].ticklabel_format(axis=\"y\", style=\"sci\", scilimits=(0,0))\n",
    "axs[2].set_title('Total')\n",
    "\n",
    "\n",
    "fig.suptitle('Distribution of Topological Depth of Actions', fontsize=16, y=1)\n",
    "fig.savefig('data_preprocessing_analysis/figures/distribution_topological_depth.png', dpi=300)\n",
    "#TODO: remove susbtations that never have any actions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
